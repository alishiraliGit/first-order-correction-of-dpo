This is the implementation of the first-order correction of DPO to align with heterogeneous preferences as explained in this paper: https://arxiv.org/abs/2502.16320

This package is now part of the paper code release here: https://github.com/arashne/dahp
